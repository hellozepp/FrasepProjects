{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the Jupyter Notebook for Analysis\n",
    "\n",
    "Note: We have our brand new package called swat - SAS Scripting Wrapper for Analytics Transfer - available on GitHub via pip install <br>\n",
    "Copyright (c) 2017 SAS Institute Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAS Viya CAS Server connection details\n",
    "\n",
    "cashost='localhost'\n",
    "casport=5570\n",
    "casauth='~/.authinfo'\n",
    "\n",
    "### Start CAS Session\n",
    "\n",
    "s = swat.CAS(cashost, casport, authinfo=casauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load actionsets for analysis (for data prep, modeling, assessing)\n",
    "actionsets = ['cardinality', 'sampling', 'fedSQL', 'decisionTree', 'regression', 'neuralNet', 'svm', 'astore','autotune']\n",
    "actions = [s.loadactionset(i) for i in actionsets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Data - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define caslib\n",
    "s.sessionprop.setsessopt(caslib='Public')\n",
    "\n",
    "# data file\n",
    "indata = 'HMEQ_NEW'\n",
    "\n",
    "# CASTable view\n",
    "castbl = s.CASTable(indata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics  import confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring data locally\n",
    "df = castbl.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation - Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hists = df.hist(figsize = (15, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set variable shortcuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable\n",
    "target = 'BAD'\n",
    "\n",
    "# get categorical/numerical variables\n",
    "input_vars = df.drop(target,axis=1).columns.values\n",
    "kinds = np.array([dt.kind for dt in df.drop(target,axis=1).dtypes])\n",
    "\n",
    "# Variable lists\n",
    "categorical_vars = list(input_vars[kinds == 'O'])\n",
    "numerical_vars = [i for i in list(input_vars[kinds != 'O']) if i not in ['Unique_ID','_PARTIND_']]\n",
    "all_vars = categorical_vars + numerical_vars + [target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation / target and inputs\n",
    "X_train = df[df['_PARTIND_'] == 1].drop(target, axis = 1)\n",
    "X_valid = df[df['_PARTIND_'] == 0].drop(target, axis = 1)\n",
    "y_train = df[df['_PARTIND_'] == 1][target]\n",
    "y_valid = df[df['_PARTIND_'] == 0][target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model - Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "     ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numerical_vars),\n",
    "    ('cat', categorical_transformer, categorical_vars)])\n",
    "\n",
    "ml_pipe = Pipeline(steps=[\n",
    "('preprocessor', preprocessor),\n",
    "('model', GradientBoostingClassifier(random_state=3))])\n",
    "\n",
    "fit = ml_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put results in pandas dataframe\n",
    "results_df = pd.DataFrame(dict(actual = y_valid, pred = ml_pipe.predict_proba(X_valid)[:,1]))\n",
    "\n",
    "# Predict and assess model\n",
    "gb_y_score           = ml_pipe.predict(X_valid)\n",
    "gb_misclassification = 1 - accuracy_score(y_valid, gb_y_score)\n",
    "gb_confusion_matrix  = confusion_matrix(y_valid, gb_y_score)\n",
    "\n",
    "# Add Python model results to CAS to assess\n",
    "pytbl = s.upload_frame(results_df, casout=dict(name='GBT_sklearn_predictions', replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis on CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "castbl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr = castbl.corr()\n",
    "\n",
    "# Visualize with Seaborn\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Models to be performed\n",
    "models = collections.OrderedDict()\n",
    "models['dt']  = 'Decision Tree'\n",
    "models['gbt'] = 'Gradient Boosting'\n",
    "models['nn']  = 'Neural Network'\n",
    "models['svm'] = 'Support Vector Machine'\n",
    "\n",
    "# Impute missing values\n",
    "castbl.dataPreprocess.impute(\n",
    "    outVarsNamePrefix = 'IMP',\n",
    "    methodContinuous  = 'MEDIAN',\n",
    "    methodNominal     = 'MODE',\n",
    "    inputs            = categorical_vars + numerical_vars,\n",
    "    copyAllVars       = True,\n",
    "    casOut            = dict(name = 'hmeq_imputed', replace = True)\n",
    ")\n",
    "castbl_imp = s.CASTable('hmeq_imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set key-word argument shortcuts (common model inputs)\n",
    "## For models that can handle missing values (decision tree, gradient boosting)\n",
    "params = dict(\n",
    "    table    = dict(name = indata, where = '_PARTIND_ = 1'), \n",
    "    target   = target, \n",
    "    inputs   = categorical_vars + numerical_vars, \n",
    "    nominals = categorical_vars,\n",
    ")\n",
    "\n",
    "## For models that can't have missing values (neural network, support vector machine)\n",
    "casinfo = castbl_imp.columninfo()['ColumnInfo']\n",
    "imp_params = dict(\n",
    "    table    = dict(name = 'hmeq_imputed', where = '_PARTIND_ = 1'), \n",
    "    target   = target, \n",
    "    inputs   = [x for x in list(casinfo['Column']) if x != target and ('IMP_' in x)], \n",
    "    nominals = [x for x in list(casinfo[casinfo.Type == 'varchar']['Column']) if x != target and ('IMP' in x)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = s.decisionTree.dtreeTrain(**params, varImp = True, casOut = dict(name = 'dt_model', replace = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#s.dropTable('gradboost_astore')\n",
    "gbt = s.decisionTree.gbtreeTrain(**params, seed = 5, casOut = dict(name = 'gbt_model', replace = True), savestate = 'gradboost_astore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = s.neuralNet.annTrain(**imp_params, seed = 1, casOut = dict(name = 'nn_model', replace = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = s.svm.svmTrain(**imp_params, seed = 1, kernel = 'polynomial', printtarget=True, id = ['_PARTIND_', 'Unique_ID'], savestate = dict(name = 'svm_model', replace = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the Models on Validation Data - CAS/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def score_model(model):\n",
    "    score = dict(\n",
    "        table      = 'hmeq_imputed',\n",
    "        modelTable = model + '_model',\n",
    "        copyVars   = [target, '_PARTIND_'],\n",
    "        casOut     = dict(name = '_scored_' + model, replace = True),\n",
    "        encodeName = True\n",
    "    )\n",
    "    return score\n",
    "\n",
    "### Decision Tree\n",
    "s.decisionTree.dtreeScore(**score_model('dt'))\n",
    "### Gradient Boosting\n",
    "s.decisionTree.gbtreeScore(**score_model('gbt'))\n",
    "### Neural Network\n",
    "s.neuralNet.annScore(**score_model('nn'))\n",
    "### Support Vector Machine\n",
    "svmscore = castbl_imp.astore.score(rstore = 'svm_model', out = dict(name = '_scored_svm_1', replace = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = s.CASTable('_scored_svm_1').merge(castbl_imp[['Unique_ID','BAD']],on='Unique_ID')\n",
    "s.dropTable('_scored_svm')\n",
    "s.table.altertable(name = joined.name, rename='_scored_svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the Performance - CAS/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model assessment function\n",
    "def assess_model(model):\n",
    "    assess = s.percentile.assess(\n",
    "        table    = dict(name = '_scored_' + model, where = '_PARTIND_ = 0'),\n",
    "        inputs   = 'p_' + target,      \n",
    "        response = target,\n",
    "        event    = '1',\n",
    "    )\n",
    "    return assess\n",
    "\n",
    "# Loop through the models and append to the roc_df dataframe\n",
    "roc_df  = pd.DataFrame()\n",
    "for i in range(len(models)):\n",
    "    tmp = assess_model(list(models)[i])\n",
    "    tmp.ROCInfo['Model'] = list(models.values())[i]\n",
    "    roc_df = pd.concat([roc_df, tmp.ROCInfo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Misclassification & ROC Curves - CAS/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the Python model using CAS\n",
    "python_assess = pytbl.percentile.assess(\n",
    "    inputs   = 'pred',      \n",
    "    response = 'actual',\n",
    "    event    = '1',   \n",
    ")\n",
    "python_assess.ROCInfo['Model'] = 'Gradient Boosting - sklearn'\n",
    "roc_df['Model'] = roc_df['Model'] + ' - CAS'\n",
    "roc_df = pd.concat([roc_df, python_assess.ROCInfo])\n",
    "roc_df['Misclassification'] = 1 - roc_df['ACC']\n",
    "\n",
    "print('\\n', 'Misclassification Rate Comparison'.center(37, ' '))\n",
    "miss = roc_df[round(roc_df['CutOff'], 2) == 0.5][['Model', 'Misclassification']].reset_index(drop = True)\n",
    "miss.sort_values('Misclassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize = (7, 6))\n",
    "for key, grp in roc_df.groupby(['Model']):\n",
    "    plt.plot(grp['FPR'], grp['Sensitivity'], label = key + ' (C = %0.2f)' % grp['C'].mean())\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Postivie Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('ROC Curve (using validation data)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dsAutoML - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.loadactionset('dataSciencePilot')\n",
    "trans_out = dict(name = 'trans_out', replace=True)\n",
    "feat_out = dict(name = 'feat_out', replace=True)\n",
    "pipeline_out = dict(name = 'pipeline_out', replace=True)\n",
    "#save_state_out = dict(name = 'save_state_out', replace=True)\n",
    "automl_model = dict(name = 'automl_model', replace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.dataSciencePilot.dsAutoMl(table=castbl, target=target,\n",
    "                            modelTypes=[\"DECISIONTREE\", \"FOREST\", \"GRADBOOST\", \"NEURALNET\"], \n",
    "                            transformationOut = trans_out,\n",
    "                            featureOut = feat_out,\n",
    "                            pipelineOut = pipeline_out,\n",
    "                            saveState = automl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.fetch(table='pipeline_out', to=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add model to repository - CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sasctl import Session\n",
    "from sasctl.tasks import register_model\n",
    "from sasctl.services import model_repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astore = s.CASTable('gradboost_astore')\n",
    "with Session('http://sasserver.demo.sas.com', 'sasdemo', 'Orion123'):\n",
    "    register_model(astore, 'GBT Model swat', '4. Manage churn models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ml_pipe\n",
    "with Session('http://sasserver.demo.sas.com', 'sasdemo', password):\n",
    "    register_model(model, 'GBT Model sklearn', '4. Manage churn models')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
