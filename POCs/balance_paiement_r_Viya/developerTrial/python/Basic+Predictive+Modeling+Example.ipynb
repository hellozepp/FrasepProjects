{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This is a simple end to end example of how you can use SAS Viya for analysis\n",
    "The example follows these steps:\n",
    "1. Importing the needed Python packages\n",
    "1. Starting a CAS session on an already running CAS server\n",
    "1. Load the needed CAS Action Sets\n",
    "1. Loading data from the local file system to the CAS server\n",
    "1. Explore the data\n",
    "1. Impute missing values\n",
    "1. Partition the data into training and validation partitions\n",
    "1. Build a decision tree\n",
    "1. Build a neural network\n",
    "1. Build a decision forest\n",
    "1. Build a gradient boost\n",
    "1. Assess the models\n",
    "1. Build ROC charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and initialize\n",
    "\n",
    "Find doc for all the CAS actions [here](http://go.documentation.sas.com/?cdcId=vdmmlcdc&cdcVersion=8.11&docsetId=caspg&docsetTarget=titlepage.htm \n",
    ") \n",
    "\n",
    "### Documentation Links:\n",
    "* [SAS® Viya™ 3.5: System Programming Guide](https://go.documentation.sas.com/?docsetId=caspg&docsetTarget=titlepage.htm&docsetVersion=3.5&locale=en)\n",
    "* [Getting Started with SAS® Viya™ 3.5 for Python](https://go.documentation.sas.com/?docsetId=caspg3&docsetTarget=titlepage.htm&docsetVersion=3.5&locale=en)\n",
    "\n",
    "In this code we import the needed packages and we assign variables for the modeling details that will be used later in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import swat\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "target          = \"bad\"\n",
    "class_inputs    = [\"reason\", \"job\"]\n",
    "class_vars      = [target] + class_inputs\n",
    "interval_inputs = [\"im_clage\", \"clno\", \"im_debtinc\", \"loan\", \"mortdue\", \"value\", \"im_yoj\", \"im_ninq\", \"derog\", \"im_delinq\"]\n",
    "all_inputs      = interval_inputs + class_inputs\n",
    "\n",
    "indata = 'hmeq'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start CAS session\n",
    "\n",
    "* Documentation to [Connect and Start a Session](http://go.documentation.sas.com/?cdcId=vdmmlcdc&cdcVersion=8.11&docsetId=caspg3&docsetTarget=home.htm&locale=en)\n",
    "\n",
    "In this code we assign values for the cashost, casport, and casauth values. These are then used to establish a CAS session named `sess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CAS_CLIENT_SSL_CA_LIST\"] = \"/opt/sas/viya/config/etc/SASSecurityCertificateFramework/cacerts/trustedcerts.pem\"\n",
    "\n",
    "cashost='controller'\n",
    "casport=5570\n",
    "sess = swat.CAS(cashost, casport)\n",
    "\n",
    "# Load the needed action sets for this example:\n",
    "sess.loadactionset('datastep')\n",
    "sess.loadactionset('datapreprocess')\n",
    "sess.loadactionset('cardinality')\n",
    "sess.loadactionset('sampling')\n",
    "sess.loadactionset('regression')\n",
    "sess.loadactionset('decisiontree')\n",
    "sess.loadactionset('neuralnet')\n",
    "sess.loadactionset('svm')\n",
    "sess.loadactionset('astore')\n",
    "sess.loadactionset('percentile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into CAS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "indata = sess.CASTable('hmeq')\n",
    "if not indata.tableexists().exists:\n",
    "    indata = sess.upload_file('http://support.sas.com/documentation/onlinedoc/viya/exampledatasets/hmeq.csv', casout=indata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "indata.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore data and plot missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tbl_data_card = sess.CASTable('data_card', replace=True)\n",
    "\n",
    "indata.cardinality.summarize(cardinality=tbl_data_card)\n",
    "\n",
    "tbl_data_card = tbl_data_card.query('_NMISS_ > 0')\n",
    "tbl_data_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_data_card['PERCENT_MISSING'] = (tbl_data_card['_NMISS_'] / tbl_data_card['_NOBS_']) * 100\n",
    "\n",
    "ax = tbl_data_card[['_VARNAME_', 'PERCENT_MISSING']].to_frame().set_index('_VARNAME_').plot.bar(\n",
    "         title='Percentage of Missing Values', figsize=(15,7)\n",
    "     )\n",
    "ax.set_ylabel('Percent Missing')\n",
    "ax.set_xlabel('Variable Names');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hmeq_prepped = sess.CASTable('hmeq_prepped', replace=True)\n",
    "\n",
    "indata.datapreprocess.transform(\n",
    "    casout=hmeq_prepped,\n",
    "    copyallvars=True,\n",
    "    outvarsnameglobalprefix='im',\n",
    "    requestpackages=[\n",
    "        {'impute': {'method': 'mean'}, 'inputs': ['clage']},\n",
    "        {'impute': {'method': 'median'}, 'inputs': ['delinq']},\n",
    "        {'impute': {'method': 'value', 'valuescontinuous': [2]}, 'inputs': ['ninq']},\n",
    "        {'impute': {'method': 'value', 'valuescontinuous': [35.0, 7, 2]}, 'inputs': ['debtinc', 'yoj']}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition data into Training and Validation\n",
    "\n",
    "The stratified action in the sampling actionset allows us to create two partition and observe the reponse rate of the target variable `bad` in both training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hmeq_part = sess.CASTable('hmeq_part', replace=True)\n",
    "\n",
    "hmeq_prepped.groupby(target).sampling.stratified(\n",
    "  output=dict(casout=hmeq_part, copyvars='all'),\n",
    "  samppct=70,\n",
    "  partind=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "In this code block we do the following:\n",
    "1. Train the decision tree using the variable listed we defined in the setup phase. We save the decision tree model `tree_model`. It is used in the subsequent step but it could just have easily been used a day, week, or month from now.\n",
    "1. Score data using the `tree_model` that was created in the previous step\n",
    "1. Run data step code on the scored output to prepare it for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hmeq_part_1 = hmeq_part.query('_partind_ = 1')\n",
    "tree_model = sess.CASTable('tree_model', replace=True)\n",
    "scored_tree = sess.CASTable('_scored_tree', replace=True)\n",
    "\n",
    "hmeq_part_1.decisiontree.dtreetrain(\n",
    "  inputs=all_inputs,\n",
    "  target='bad',\n",
    "  nominals=class_vars,\n",
    "  crit='gain',\n",
    "  prune=True,\n",
    "  varImp=True,\n",
    "  missing='useinsearch',\n",
    "  casout=tree_model\n",
    ")\n",
    "\n",
    "# Score \n",
    "hmeq_part.decisiontree.dtreescore(\n",
    "  modeltable=tree_model,\n",
    "  casout=scored_tree,\n",
    "  copyvars=['bad', '_partind_']\n",
    ")\n",
    "\n",
    "# Create p_bad0 and p_bad1 as _dt_predp_ is the probability of event in _dt_predname_\n",
    "scored_tree['p_bad1'] = scored_tree.eval(\"ifn( strip(_dt_predname_) = '1', _dt_predp_, 1-_dt_predp_ )\") \n",
    "scored_tree['p_bad0'] = scored_tree.eval(\"ifn( strip(_dt_predname_) = '0', 1-_dt_predp_, _dt_predp_ )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Forest\n",
    "\n",
    "In this code block we do the following:\n",
    "1. Train the decision tree using the variable listed we defined in the setup phase. We save the decision tree model `forest_model`. It is used in the subsequent step but it could just have easily been used a day, week, or month from now.\n",
    "1. Score data using the `forest_model` that was created in the previous step\n",
    "1. Run data step code on the scored output to prepare it for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "forest_model = sess.CASTable('forest_model', replace=True)\n",
    "scored_rf = sess.CASTable('_scored_rf', replace=True)\n",
    "\n",
    "hmeq_part_1.decisiontree.foresttrain(\n",
    "  inputs=all_inputs,\n",
    "  nominals=class_vars,\n",
    "  target='bad',\n",
    "  ntree=50,\n",
    "  nbins=20,\n",
    "  leafsize=5,\n",
    "  maxlevel=21,\n",
    "  crit='gainratio',\n",
    "  varimp=True,\n",
    "  missing='useinsearch',\n",
    "  vote='prob',\n",
    "  casout=forest_model\n",
    ")\n",
    "\n",
    "# Score \n",
    "hmeq_part.decisiontree.forestscore(\n",
    "  modeltable=forest_model,\n",
    "  casout=scored_rf,\n",
    "  copyvars=['bad', '_partind_'],\n",
    "  vote='prob'\n",
    ")\n",
    "\n",
    "# Create p_bad0 and p_bad1 as _rf_predp_ is the probability of event in _rf_predname_\n",
    "scored_rf['p_bad1'] = scored_rf.eval(\"ifn( strip(_rf_predname_) = '1', _rf_predp_, 1-_rf_predp_ )\") \n",
    "scored_rf['p_bad0'] = scored_rf.eval(\"ifn( strip(_rf_predname_) = '0', 1-_rf_predp_, _rf_predp_ )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine\n",
    "\n",
    "\n",
    "In this code block we do the following:\n",
    "1. Train the decision tree using the variable listed we defined in the setup phase. We save the decision tree model `gb_model`. It is used in the subsequent step but it could just have easily been used a day, week, or month from now.\n",
    "1. Score data using the `gb_model` that was created in the previous step\n",
    "1. Run data step code on the scored output to prepare it for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "gb_model = sess.CASTable('gb_model', replace=True)\n",
    "scored_gb = sess.CASTable('_scored_gb', replace=True)\n",
    "\n",
    "hmeq_part_1.decisiontree.gbtreetrain(\n",
    "  inputs=all_inputs,\n",
    "  nominals=class_vars,\n",
    "  target=target,\n",
    "  ntree=10,\n",
    "  nbins=20,\n",
    "  maxlevel=6,\n",
    "  varimp=True,\n",
    "  missing='useinsearch',\n",
    "  casout=gb_model\n",
    ")\n",
    "\n",
    "# Score \n",
    "hmeq_part.decisionTree.gbtreeScore(\n",
    "  modeltable=gb_model,\n",
    "  casout=scored_gb,\n",
    "  copyvars=[target, '_partind_']\n",
    ")\n",
    "\n",
    "# Create p_bad0 and p_bad1 as _gbt_predp_ is the probability of event in _gbt_predname_\n",
    "scored_gb['p_bad1'] = scored_gb.eval(\"ifn( strip(_gbt_predname_) = '1', _gbt_predp_, 1-_gbt_predp_ )\") \n",
    "scored_gb['p_bad0'] = scored_gb.eval(\"ifn( strip(_gbt_predname_) = '0', 1-_gbt_predp_, _gbt_predp_ )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "In this code block we do the following:\n",
    "1. Train the decision tree using the variable listed we defined in the setup phase. We save the decision tree model `nnet_model`. It is used in the subsequent step but it could just have easily been used a day, week, or month from now.\n",
    "1. Score data using the `nnet_model` that was created in the previous step\n",
    "1. Run data step code on the scored output to prepare it for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hmeq_part_0 = hmeq_part.query('_partind_ = 0')\n",
    "nnet_model = sess.CASTable('nnet_model', replace=True)\n",
    "scored_nn = sess.CASTable('_scored_nn', replace=True)\n",
    "\n",
    "hmeq_part_1.neuralnet.anntrain(\n",
    "  validtable=hmeq_part_0,\n",
    "  inputs=all_inputs,\n",
    "  nominals=class_vars,\n",
    "  target=\"bad\",\n",
    "  hiddens={9},\n",
    "  acts=['tanh'],\n",
    "  combs=['linear'],\n",
    "  targetact='softmax',\n",
    "  errorfunc='entropy',\n",
    "  std='midrange',\n",
    "  randdist='uniform',\n",
    "  scaleinit=1,\n",
    "  nloopts={\n",
    "      'optmlopt': {'maxiters': 250, 'fconv': 1e-10}, \n",
    "      'lbfgsopt': {'numcorrections': 6},\n",
    "      'printopt': {'printlevel': 'printdetail'},\n",
    "      'validate': {'frequency': 1}\n",
    "  },\n",
    "  casout=nnet_model\n",
    ")\n",
    "\n",
    "# Score \n",
    "hmeq_part.neuralnet.annscore(\n",
    "  modeltable=nnet_model,\n",
    "  casout=scored_nn,\n",
    "  copyvars=['bad', '_partind_']\n",
    ")\n",
    "\n",
    "# Create p_bad0 and p_bad1 as _nn_predp_ is the probability of event in _nn_predname_\n",
    "scored_nn['p_bad1'] = scored_nn.eval(\"ifn( strip(_nn_predname_) = '1', _nn_predp_, 1-_nn_predp_ )\") \n",
    "scored_nn['p_bad0'] = scored_nn.eval(\"ifn( strip(_nn_predname_) = '0', 1-_nn_predp_, _nn_predp_ )\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def assess_model(t):\n",
    "    return sess.percentile.assess(\n",
    "               table=t.query('_partind_ = 0'),\n",
    "               inputs=['p_bad1'],      \n",
    "               response='bad',\n",
    "               event='1',\n",
    "               pvar=['p_bad0'],\n",
    "               pevent=['0']      \n",
    "           )\n",
    "\n",
    "tree_assess   = assess_model(scored_tree)    \n",
    "tree_fitstat  = tree_assess.FitStat\n",
    "tree_rocinfo  = tree_assess.ROCInfo\n",
    "tree_liftinfo = tree_assess.LIFTInfo\n",
    "\n",
    "rf_assess   = assess_model(scored_rf)    \n",
    "rf_fitstat  = rf_assess.FitStat\n",
    "rf_rocinfo  = rf_assess.ROCInfo\n",
    "rf_liftinfo = rf_assess.LIFTInfo\n",
    "\n",
    "gb_assess   = assess_model(scored_gb)    \n",
    "gb_fitstat  = gb_assess.FitStat\n",
    "gb_rocinfo  = gb_assess.ROCInfo\n",
    "gb_liftinfo = gb_assess.LIFTInfo\n",
    "\n",
    "nn_assess   = assess_model(scored_nn)    \n",
    "nn_fitstat  = nn_assess.FitStat\n",
    "nn_rocinfo  = nn_assess.ROCInfo\n",
    "nn_liftinfo = nn_assess.LIFTInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ROC and Lift plots (using Validation data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare assessment results for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Add new variable to indicate type of model\n",
    "tree_liftinfo['model'] = 'DecisionTree'\n",
    "tree_rocinfo['model']  = 'DecisionTree'\n",
    "rf_liftinfo['model']   = 'Forest'\n",
    "rf_rocinfo['model']    = 'Forest'\n",
    "gb_liftinfo['model']   = 'GradientBoosting'\n",
    "gb_rocinfo['model']    = 'GradientBoosting'\n",
    "nn_liftinfo['model']   = 'NeuralNetwork'\n",
    "nn_rocinfo['model']    = 'NeuralNetwork'\n",
    "\n",
    "# Concatenate data\n",
    "all_liftinfo = pd.concat([rf_liftinfo, gb_liftinfo, nn_liftinfo, tree_liftinfo], ignore_index=True)\n",
    "all_rocinfo = pd.concat([rf_rocinfo, gb_rocinfo, nn_rocinfo, tree_rocinfo], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print AUC (Area Under the ROC Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_rocinfo[['model', 'C']].drop_duplicates(keep='first').sort_values(by='C', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw ROC and Lift plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Draw ROC charts \n",
    "plt.figure(figsize=(15, 5))\n",
    "for key, grp in all_rocinfo.groupby(['model']):\n",
    "    plt.plot(grp['FPR'], grp['Sensitivity'], label=key)\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "# Draw lift charts\n",
    "plt.figure(figsize=(15, 5))\n",
    "for key, grp in all_liftinfo.groupby(['model']):\n",
    "    plt.plot(grp['Depth'], grp['CumLift'], label=key)\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Cumulative Lift')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Cumulative Lift Chart')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End CAS session\n",
    "\n",
    "This closes the CAS session freeing resources for others to leverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This is the same as sess.endsession(); sess.close();\n",
    "sess.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
